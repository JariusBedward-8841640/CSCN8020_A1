{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create updated gridworld environment",
   "id": "f2d5822e246d25fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 1: Updating the Reward Function\n",
    "- In an MDP, the reward function is formally defined as a mapping\n",
    "\n",
    "- Instead of pre-filling a reward matrix we update the code by implementing the rule directly into the environment to reflect the mathematical definition\n",
    "\n",
    "- The reward is implemented dynamically based on state category instead of storing a fixed reward grid. This is done to make sure that there is a clean separation between environment structure and reward logic as well as easy modification if reward values change"
   ],
   "id": "ff034d5426ca659d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridWorld():\n",
    "    def __init__(self, env_size):\n",
    "        self.env_size = env_size\n",
    "\n",
    "        # Define special states\n",
    "        self.terminal_state = (4, 4)\n",
    "        self.grey_states = [(2, 2), (3, 0), (0, 4)]\n",
    "\n",
    "        # Reward values based on state category\n",
    "        self.terminal_reward = 10\n",
    "        self.grey_reward = -5\n",
    "        self.regular_reward = -1\n",
    "\n",
    "        # Define possible actions: Right, Left, Down, Up\n",
    "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "        self.action_description = [\"Right\", \"Left\", \"Down\", \"Up\"]\n",
    "\n",
    "    def get_reward(self, i, j):\n",
    "        \"\"\"\n",
    "        Returns reward based on the type of state.\n",
    "        Implements R(s) according to assignment specification.\n",
    "        \"\"\"\n",
    "        if (i, j) == self.terminal_state:\n",
    "            return self.terminal_reward\n",
    "        elif (i, j) in self.grey_states:\n",
    "            return self.grey_reward\n",
    "        else:\n",
    "            return self.regular_reward\n",
    "\n",
    "    def step(self, action_index, i, j):\n",
    "        \"\"\"\n",
    "        Deterministic transition function.\n",
    "        If action is invalid (off-grid), the agent remains in the same state.\n",
    "        \"\"\"\n",
    "        action = self.actions[action_index]\n",
    "        next_i, next_j = i + action[0], j + action[1]\n",
    "\n",
    "        # Boundary check\n",
    "        if not self.is_valid_state(next_i, next_j):\n",
    "            next_i, next_j = i, j\n",
    "\n",
    "        reward = self.get_reward(next_i, next_j)\n",
    "        done = self.is_terminal_state(next_i, next_j)\n",
    "\n",
    "        return next_i, next_j, reward, done\n",
    "\n",
    "    def is_valid_state(self, i, j):\n",
    "        return 0 <= i < self.env_size and 0 <= j < self.env_size\n",
    "\n",
    "    def is_terminal_state(self, i, j):\n",
    "        return (i, j) == self.terminal_state\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.env_size\n",
    "\n",
    "    def get_actions(self):\n",
    "        return self.actions\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 1.2 Run the code\n",
    "- Using the newly updated reward function from task 1 the same value iteration code from in class\n",
    "\n",
    "- Since transitions are deterministic,\n",
    "\n",
    "- We iterate until convergence and then extrac the optimal policy by selecting the action that maximizes the BellMan expression at each state"
   ],
   "id": "39ff882500dd35a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Parameters\n",
    "gamma = 0.9\n",
    "theta = 1e-6\n",
    "\n",
    "env = GridWorld(5)\n",
    "\n",
    "# Initialize value table\n",
    "V = np.zeros((env.get_size(), env.get_size()))\n",
    "\n",
    "converged = False\n",
    "iterations = 0\n",
    "\n",
    "while not converged:\n",
    "    delta = 0\n",
    "    new_V = np.copy(V)\n",
    "\n",
    "    for i in range(env.get_size()):\n",
    "        for j in range(env.get_size()):\n",
    "\n",
    "            # Skip terminal state\n",
    "            if env.is_terminal_state(i, j):\n",
    "                continue\n",
    "\n",
    "            action_values = []\n",
    "\n",
    "            for a in range(len(env.get_actions())):\n",
    "                next_i, next_j, reward, done = env.step(a, i, j)\n",
    "                action_value = reward + gamma * V[next_i, next_j]\n",
    "                action_values.append(action_value)\n",
    "\n",
    "            best_value = max(action_values)\n",
    "            new_V[i, j] = best_value\n",
    "\n",
    "            delta = max(delta, abs(V[i, j] - new_V[i, j]))\n",
    "\n",
    "    V = new_V\n",
    "    iterations += 1\n",
    "\n",
    "    if delta < theta:\n",
    "        converged = True\n",
    "\n",
    "print(\"Converged in\", iterations, \"iterations\")\n",
    "\n",
    "# Extract optimal policy\n",
    "policy = np.empty((env.get_size(), env.get_size()), dtype=object)\n",
    "\n",
    "for i in range(env.get_size()):\n",
    "    for j in range(env.get_size()):\n",
    "\n",
    "        if env.is_terminal_state(i, j):\n",
    "            policy[i, j] = \"G\"\n",
    "            continue\n",
    "\n",
    "        action_values = []\n",
    "\n",
    "        for a in range(len(env.get_actions())):\n",
    "            next_i, next_j, reward, done = env.step(a, i, j)\n",
    "            action_value = reward + gamma * V[next_i, next_j]\n",
    "            action_values.append(action_value)\n",
    "\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy[i, j] = env.action_description[best_action]\n",
    "\n",
    "print(\"\\nOptimal Value Function (V*):\")\n",
    "print(np.round(V, 2))\n",
    "\n",
    "print(\"\\nOptimal Policy (Ï€*):\")\n",
    "print(policy)\n"
   ],
   "id": "ff814ff7c2bcac2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "71e24ad990c13a26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5f6686079863f645"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "50a8a4ec131ad984"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
