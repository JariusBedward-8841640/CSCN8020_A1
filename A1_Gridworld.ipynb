{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Problem 3 5X5 Grid Reward:\n",
    "Create updated gridworld environment"
   ],
   "id": "f2d5822e246d25fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 1: Updating the Reward Function\n",
    "- In an MDP, the reward function is formally defined as a mapping\n",
    "\n",
    "- Instead of pre-filling a reward matrix we update the code by implementing the rule directly into the environment to reflect the mathematical definition\n",
    "\n",
    "- The reward is implemented dynamically based on state category instead of storing a fixed reward grid. This is done to make sure that there is a clean separation between environment structure and reward logic as well as easy modification if reward values change"
   ],
   "id": "ff034d5426ca659d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-11T01:26:24.555352Z",
     "start_time": "2026-02-11T01:26:24.527863Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridWorld():\n",
    "    def __init__(self, env_size):\n",
    "        self.env_size = env_size\n",
    "\n",
    "        #Define special states\n",
    "        self.terminal_state = (4,4)\n",
    "        self.grey_states = [(2,2), (3,0), (0,4)]\n",
    "\n",
    "        #Reward values based on state category\n",
    "        self.terminal_reward = 10\n",
    "        self.grey_reward = -5\n",
    "        self.regular_reward = -1\n",
    "\n",
    "        #Define possible actions: Right, Left, Down, Up\n",
    "\n",
    "        self.actions = [(0,1), (0,-1), (1,0), (-1,0)]\n",
    "        self.action_description = [\"Right\", \"Left\", \"Down\",\"Up\"]\n",
    "\n",
    "\n",
    "    def get_reward(self, i, j):\n",
    "         #Returns reward based on type of state. Implements R(s)\n",
    "        if (i,j) == self.terminal_state:\n",
    "            return self.terminal_reward\n",
    "        elif(i,j) in self.grey_states:\n",
    "            return self.grey_reward\n",
    "        else:\n",
    "            return self.regular_reward\n",
    "\n",
    "    def step (self, action_index, i, j):\n",
    "        #Deterministic transition function, If action is invalid the agent remains in the same state\n",
    "        action = self.actions[action_index]\n",
    "        next_i, next_j = i + action[0], j + action[1]\n",
    "\n",
    "        #Boundary Check\n",
    "        if not self.is_valid_state(next_i, next_j):\n",
    "            next_i, next_j = i, j\n",
    "\n",
    "        reward = self.get_reward(next_i, next_j)\n",
    "        done = self.is_terminal_state(next_i, next_j)\n",
    "\n",
    "        return next_i, next_j, reward, done\n",
    "\n",
    "    def is_valid_state(self):\n",
    "        return 0 <= i < self.env_size and 0 <= j < self.env_size\n",
    "    def is_terminal_state(self, i, j):\n",
    "        return (i,j) == self.terminal_state\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.env_size\n",
    "    def get_actions(self):\n",
    "        return self.actions"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The step cost being -1 encourages the shortest path because longer trajectories accumulate more negative reward.\n",
    "\n",
    "The -5 penalty in the grey states creates local depressions in the value landscape which forces the optimal policy to consider trade offs between shorter paths and higher penalties"
   ],
   "id": "cdbbab8ef8a4133b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 1.2 Run the code\n",
    "- Using the newly updated reward function from task 1 the same value iteration code from in class\n",
    "\n",
    "- Since transitions are deterministic,\n",
    "\n",
    "- We iterate until convergence and then extrac the optimal policy by selecting the action that maximizes the BellMan expression at each state"
   ],
   "id": "39ff882500dd35a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T01:26:24.599729Z",
     "start_time": "2026-02-11T01:26:24.572733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parameters\n",
    "gamma = 0.9\n",
    "theta = 1e-6\n",
    "\n",
    "env = GridWorld(5)\n",
    "\n",
    "# Initialize value table\n",
    "V = np.zeros((env.get_size(), env.get_size()))\n",
    "\n",
    "converged = False\n",
    "iterations = 0\n",
    "\n",
    "while not converged:\n",
    "    delta = 0\n",
    "    new_V = np.copy(V)\n",
    "\n",
    "    for i in range(env.get_size()):\n",
    "        for j in range(env.get_size()):\n",
    "\n",
    "            # Skip terminal state\n",
    "            if env.is_terminal_state(i, j):\n",
    "                continue\n",
    "\n",
    "            action_values = []\n",
    "\n",
    "            for a in range(len(env.get_actions())):\n",
    "                next_i, next_j, reward, done = env.step(a, i, j)\n",
    "                action_value = reward + gamma * V[next_i, next_j]\n",
    "                action_values.append(action_value)\n",
    "\n",
    "            best_value = max(action_values)\n",
    "            new_V[i, j] = best_value\n",
    "\n",
    "            delta = max(delta, abs(V[i, j] - new_V[i, j]))\n",
    "\n",
    "    V = new_V\n",
    "    iterations += 1\n",
    "\n",
    "    if delta < theta:\n",
    "        converged = True\n",
    "\n",
    "print(\"Converged in\", iterations, \"iterations\")\n",
    "\n",
    "# Extract optimal policy\n",
    "policy = np.empty((env.get_size(), env.get_size()), dtype=object)\n",
    "\n",
    "for i in range(env.get_size()):\n",
    "    for j in range(env.get_size()):\n",
    "\n",
    "        if env.is_terminal_state(i, j):\n",
    "            policy[i, j] = \"G\"\n",
    "            continue\n",
    "\n",
    "        action_values = []\n",
    "\n",
    "        for a in range(len(env.get_actions())):\n",
    "            next_i, next_j, reward, done = env.step(a, i, j)\n",
    "            action_value = reward + gamma * V[next_i, next_j]\n",
    "            action_values.append(action_value)\n",
    "\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy[i, j] = env.action_description[best_action]\n",
    "\n",
    "print(\"\\nOptimal Value Function (V*):\")\n",
    "print(np.round(V, 2))\n",
    "\n",
    "print(\"\\nOptimal Policy (π*):\")\n",
    "print(policy)\n"
   ],
   "id": "ff814ff7c2bcac2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 9 iterations\n",
      "\n",
      "Optimal Value Function (V*):\n",
      "[[-0.43  0.63  1.81  3.12  4.58]\n",
      " [ 0.63  1.81  3.12  4.58  6.2 ]\n",
      " [ 1.81  3.12  4.58  6.2   8.  ]\n",
      " [ 3.12  4.58  6.2   8.   10.  ]\n",
      " [ 4.58  6.2   8.   10.    0.  ]]\n",
      "\n",
      "Optimal Policy (π*):\n",
      "[['Right' 'Right' 'Right' 'Down' 'Down']\n",
      " ['Right' 'Right' 'Right' 'Right' 'Down']\n",
      " ['Right' 'Down' 'Right' 'Right' 'Down']\n",
      " ['Right' 'Right' 'Right' 'Right' 'Down']\n",
      " ['Right' 'Right' 'Right' 'Right' 'G']]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 2 In-Place Value Iteration\n",
    "- in standard Value Iteration two arrays are maintained\n",
    "  - Vk (old Values)\n",
    "  - Vk +1 (New Values)\n",
    "- All updates are computed using only the previous iterations values this is known as a synchronous update\n"
   ],
   "id": "b18cfee67b7a4563"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T01:26:24.640021Z",
     "start_time": "2026-02-11T01:26:24.617247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "# -------------------------------\n",
    "# Task 2: In-Place Value Iteration\n",
    "# -------------------------------\n",
    "\n",
    "V_inplace = np.zeros_like(V)\n",
    "iterations_inplace = 0\n",
    "converged = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while not converged:\n",
    "    delta = 0\n",
    "\n",
    "    for i in range(env.get_size()):\n",
    "        for j in range(env.get_size()):\n",
    "\n",
    "            if env.is_terminal_state(i, j):\n",
    "                continue\n",
    "\n",
    "            old_value = V_inplace[i, j]\n",
    "\n",
    "            action_values = []\n",
    "\n",
    "            for a in range(len(env.get_actions())):\n",
    "                next_i, next_j, reward, done = env.step(a, i, j)\n",
    "                action_value = reward + gamma * V_inplace[next_i, next_j]\n",
    "                action_values.append(action_value)\n",
    "\n",
    "            V_inplace[i, j] = max(action_values)\n",
    "\n",
    "            delta = max(delta, abs(old_value - V_inplace[i, j]))\n",
    "\n",
    "    iterations_inplace += 1\n",
    "\n",
    "    if delta < theta:\n",
    "        converged = True\n",
    "\n",
    "end_time = time.time()\n",
    "runtime_inplace = end_time - start_time\n",
    "\n",
    "# -------------------------------\n",
    "# Extract Policy from In-Place V*\n",
    "# -------------------------------\n",
    "\n",
    "policy_inplace = np.empty((env.get_size(), env.get_size()), dtype=object)\n",
    "\n",
    "for i in range(env.get_size()):\n",
    "    for j in range(env.get_size()):\n",
    "\n",
    "        if env.is_terminal_state(i, j):\n",
    "            policy_inplace[i, j] = \"G\"\n",
    "            continue\n",
    "\n",
    "        action_values = []\n",
    "\n",
    "        for a in range(len(env.get_actions())):\n",
    "            next_i, next_j, reward, done = env.step(a, i, j)\n",
    "            action_value = reward + gamma * V_inplace[next_i, next_j]\n",
    "            action_values.append(action_value)\n",
    "\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy_inplace[i, j] = env.action_description[best_action]\n",
    "\n",
    "# -------------------------------\n",
    "# Comparison Results\n",
    "# -------------------------------\n",
    "\n",
    "print(\"In-Place VI Converged in\", iterations_inplace, \"iterations\")\n",
    "print(\"Runtime:\", round(runtime_inplace, 6), \"seconds\")\n",
    "\n",
    "print(\"\\nOptimal Value Function (In-Place V*):\")\n",
    "print(np.round(V_inplace, 2))\n",
    "\n",
    "print(\"\\nMaximum difference between standard and in-place V*:\",\n",
    "      np.max(np.abs(V - V_inplace)))\n",
    "\n",
    "print(\"\\nDo policies differ?\",\n",
    "      np.any(policy != policy_inplace))\n"
   ],
   "id": "863b1ead93bfc4bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-Place VI Converged in 9 iterations\n",
      "Runtime: 0.003993 seconds\n",
      "\n",
      "Optimal Value Function (In-Place V*):\n",
      "[[-0.43  0.63  1.81  3.12  4.58]\n",
      " [ 0.63  1.81  3.12  4.58  6.2 ]\n",
      " [ 1.81  3.12  4.58  6.2   8.  ]\n",
      " [ 3.12  4.58  6.2   8.   10.  ]\n",
      " [ 4.58  6.2   8.   10.    0.  ]]\n",
      "\n",
      "Maximum difference between standard and in-place V*: 0.0\n",
      "\n",
      "Do policies differ? False\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The in-place variation converges to the same optimal value function as the standard synchronous Value iteration\n",
    "\n",
    "The maximum difference between the two computed value functions is approximately zero(within numerical tolerance, confirming correctness\n",
    "\n",
    "However, the number of iterations and runtime amy differ. In place updates often propagate value information faster, potentially reducing convergence time while maintain the same computational complexity per iteration\n",
    "This means both methods react the same fixed point V* but their convergence behaviour differs due to update ordering\n"
   ],
   "id": "2fab63d187dc6193"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Problem 4: Off Policy Monte Carlo",
   "id": "c54c89ab2250ff04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- We generate episodes using a behavior policy and evaluate a different target policy\n",
    "- Because episodes are generated under b, but evaluated under policy, we must correct the distribution mismatch using importance sampling\n",
    "\n",
    "- We use off policy to allow for the learning of optimal policy while exploring randomly\n",
    "\n",
    "- It separates exploration from exploitation\n",
    "\n",
    "- It is more flexible than other on policy methods because off policy allows the agent to learn about the target policy while behaving according to another policy the behavior which enables the agent to explore safely or use previously collected data while still converging toward the optimal greedy policy"
   ],
   "id": "9ae14e78b8ff348a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T01:26:26.102058Z",
     "start_time": "2026-02-11T01:26:24.667035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "gamma = 0.9\n",
    "num_episodes = 5000\n",
    "\n",
    "# Behavior policy: random\n",
    "def behavior_policy():\n",
    "    return random.choice(range(len(env.get_actions())))\n",
    "\n",
    "# Initialize\n",
    "V_mc = np.zeros((env.get_size(), env.get_size()))\n",
    "C = np.zeros_like(V_mc)\n",
    "\n",
    "# Greedy target policy initialized arbitrarily\n",
    "policy_mc = np.zeros((env.get_size(), env.get_size()), dtype=int)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "\n",
    "    # Generate an episode\n",
    "    episode_data = []\n",
    "\n",
    "    i, j = random.randint(0,4), random.randint(0,4)\n",
    "\n",
    "    while not env.is_terminal_state(i, j):\n",
    "        action = behavior_policy()\n",
    "        next_i, next_j, reward, done = env.step(action, i, j)\n",
    "\n",
    "        episode_data.append((i, j, action, reward))\n",
    "\n",
    "        i, j = next_i, next_j\n",
    "\n",
    "    G = 0\n",
    "    W = 1\n",
    "\n",
    "    # Traverse episode backwards\n",
    "    for t in reversed(range(len(episode_data))):\n",
    "        state_i, state_j, action, reward = episode_data[t]\n",
    "\n",
    "        G = gamma * G + reward\n",
    "\n",
    "        C[state_i, state_j] += W\n",
    "\n",
    "        V_mc[state_i, state_j] += (W / C[state_i, state_j]) * (G - V_mc[state_i, state_j])\n",
    "\n",
    "        # Improve target policy greedily\n",
    "        action_values = []\n",
    "        for a in range(len(env.get_actions())):\n",
    "            next_i, next_j, r, _ = env.step(a, state_i, state_j)\n",
    "            action_values.append(r + gamma * V_mc[next_i, next_j])\n",
    "\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy_mc[state_i, state_j] = best_action\n",
    "\n",
    "        if action != best_action:\n",
    "            break\n",
    "\n",
    "        W *= 1.0 / 0.25  # since behavior policy is uniform random (1/4)\n",
    "\n",
    "end_time = time.time()\n",
    "runtime_mc = end_time - start_time\n",
    "\n",
    "print(\"Monte Carlo completed.\")\n",
    "print(\"Episodes:\", num_episodes)\n",
    "print(\"Runtime:\", round(runtime_mc, 4), \"seconds\")\n",
    "\n",
    "print(\"\\nEstimated Value Function (Monte Carlo):\")\n",
    "print(np.round(V_mc, 2))\n",
    "\n"
   ],
   "id": "71e24ad990c13a26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo completed.\n",
      "Episodes: 5000\n",
      "Runtime: 1.4151 seconds\n",
      "\n",
      "Estimated Value Function (Monte Carlo):\n",
      "[[0.   0.63 1.81 0.96 1.2 ]\n",
      " [0.   1.09 0.49 3.01 3.69]\n",
      " [1.81 0.36 3.94 3.55 5.92]\n",
      " [0.21 4.58 3.67 6.35 7.63]\n",
      " [0.   5.89 5.58 7.54 0.  ]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Behavior Policy\n",
    "A uniform random policy was used: b(a∣s)= 1/4\n",
    "This ensures sufficient exploration of the state space\n"
   ],
   "id": "1fd152f15635c556"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Greedy Target Policy\n",
    "The target policy is greedy with respect ot the current value estimates\n",
    "\n",
    "This drives convergence toward optimal behavior"
   ],
   "id": "78a553d37440e46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Weighted Importance Sampling\n",
    "Weighted  importance sampling was used because\n",
    "It reduces variance, provides stable convergence and ensures unbiased estimation"
   ],
   "id": "93346ccf5b67f1b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Comparison with Value Iteration",
   "id": "c0c515af11b7cf4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T01:26:26.216800Z",
     "start_time": "2026-02-11T01:26:26.208383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nComparison with Value Iteration\")\n",
    "\n",
    "print(\"Value Iteration Runtime:\", round (runtime_inplace, 6), \"seconds\")\n",
    "print(\"Monte Carlo Runtime:\", round(runtime_mc, 6), \"seconds\")\n",
    "\n",
    "difference_mc_vi = np.max(np.abs(V_mc - V_mc))\n",
    "print(\"Maximum difference between VI and MC value functions\", difference_mc_vi)\n"
   ],
   "id": "5f6686079863f645",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison with Value Iteration\n",
      "Value Iteration Runtime: 0.003993 seconds\n",
      "Monte Carlo Runtime: 1.415107 seconds\n",
      "Maximum difference between VI and MC value functions: 4.58\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Comparison Interpretation\n",
    "\n",
    "1. Optimization Time\n",
    "    - Value Iteration: Deterministic Updates, converges quickly, typically faster for small state spaces\n",
    "    - Monte Carlo: Requires many episodes, slower convergence, runtime depends on number of episodes\n",
    "2. Computational CComplexity\n",
    "    -\n",
    "3. Stability and Convergence\n",
    "    - Value Iteration: Guaranteed convergence\n",
    "    - Monte Carlo: Converges asymptotically, higher variance, sensitive to number of episodes\n",
    "4. Observations\n",
    "    - Monte Carlo Estimates approach the value iteration solution as episode count increases\n",
    "    - Value iteration is more efficient for small, known environments\n",
    "    - Monte Carlo is useful when transition probabilities are unknown\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "c164a885d8c434c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T01:26:26.286395Z",
     "start_time": "2026-02-11T01:26:26.282350Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "50a8a4ec131ad984",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
