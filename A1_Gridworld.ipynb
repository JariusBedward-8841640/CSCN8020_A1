{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create updated gridworld environment",
   "id": "f2d5822e246d25fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 1: Updating the Reward Function\n",
    "- In an MDP, the reward function is formally defined as a mapping\n",
    "\n",
    "- Instead of pre-filling a reward matrix we update the code by implementing the rule directly into the environment to reflect the mathematical definition\n",
    "\n",
    "- The reward is implemented dynamically based on state category instead of storing a fixed reward grid. This is done to make sure that there is a clean separation between environment structure and reward logic as well as easy modification if reward values change"
   ],
   "id": "ff034d5426ca659d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-11T00:04:27.270597Z",
     "start_time": "2026-02-11T00:04:27.258263Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridWorld():\n",
    "    def __init__(self, env_size):\n",
    "        self.env_size = env_size\n",
    "\n",
    "        # Define special states\n",
    "        self.terminal_state = (4, 4)\n",
    "        self.grey_states = [(2, 2), (3, 0), (0, 4)]\n",
    "\n",
    "        # Reward values based on state category\n",
    "        self.terminal_reward = 10\n",
    "        self.grey_reward = -5\n",
    "        self.regular_reward = -1\n",
    "\n",
    "        # Define possible actions: Right, Left, Down, Up\n",
    "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "        self.action_description = [\"Right\", \"Left\", \"Down\", \"Up\"]\n",
    "\n",
    "    def get_reward(self, i, j):\n",
    "        \"\"\"\n",
    "        Returns reward based on the type of state.\n",
    "        Implements R(s) according to assignment specification.\n",
    "        \"\"\"\n",
    "        if (i, j) == self.terminal_state:\n",
    "            return self.terminal_reward\n",
    "        elif (i, j) in self.grey_states:\n",
    "            return self.grey_reward\n",
    "        else:\n",
    "            return self.regular_reward\n",
    "\n",
    "    def step(self, action_index, i, j):\n",
    "        \"\"\"\n",
    "        Deterministic transition function.\n",
    "        If action is invalid (off-grid), the agent remains in the same state.\n",
    "        \"\"\"\n",
    "        action = self.actions[action_index]\n",
    "        next_i, next_j = i + action[0], j + action[1]\n",
    "\n",
    "        # Boundary check\n",
    "        if not self.is_valid_state(next_i, next_j):\n",
    "            next_i, next_j = i, j\n",
    "\n",
    "        reward = self.get_reward(next_i, next_j)\n",
    "        done = self.is_terminal_state(next_i, next_j)\n",
    "\n",
    "        return next_i, next_j, reward, done\n",
    "\n",
    "    def is_valid_state(self, i, j):\n",
    "        return 0 <= i < self.env_size and 0 <= j < self.env_size\n",
    "\n",
    "    def is_terminal_state(self, i, j):\n",
    "        return (i, j) == self.terminal_state\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.env_size\n",
    "\n",
    "    def get_actions(self):\n",
    "        return self.actions\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 1.2 Run the code\n",
    "- Using the newly updated reward function from task 1 the same value iteration code from in class\n",
    "\n",
    "- Since transitions are deterministic,\n",
    "\n",
    "- We iterate until convergence and then extrac the optimal policy by selecting the action that maximizes the BellMan expression at each state"
   ],
   "id": "39ff882500dd35a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:04:27.332540Z",
     "start_time": "2026-02-11T00:04:27.312216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parameters\n",
    "gamma = 0.9\n",
    "theta = 1e-6\n",
    "\n",
    "env = GridWorld(5)\n",
    "\n",
    "# Initialize value table\n",
    "V = np.zeros((env.get_size(), env.get_size()))\n",
    "\n",
    "converged = False\n",
    "iterations = 0\n",
    "\n",
    "while not converged:\n",
    "    delta = 0\n",
    "    new_V = np.copy(V)\n",
    "\n",
    "    for i in range(env.get_size()):\n",
    "        for j in range(env.get_size()):\n",
    "\n",
    "            # Skip terminal state\n",
    "            if env.is_terminal_state(i, j):\n",
    "                continue\n",
    "\n",
    "            action_values = []\n",
    "\n",
    "            for a in range(len(env.get_actions())):\n",
    "                next_i, next_j, reward, done = env.step(a, i, j)\n",
    "                action_value = reward + gamma * V[next_i, next_j]\n",
    "                action_values.append(action_value)\n",
    "\n",
    "            best_value = max(action_values)\n",
    "            new_V[i, j] = best_value\n",
    "\n",
    "            delta = max(delta, abs(V[i, j] - new_V[i, j]))\n",
    "\n",
    "    V = new_V\n",
    "    iterations += 1\n",
    "\n",
    "    if delta < theta:\n",
    "        converged = True\n",
    "\n",
    "print(\"Converged in\", iterations, \"iterations\")\n",
    "\n",
    "# Extract optimal policy\n",
    "policy = np.empty((env.get_size(), env.get_size()), dtype=object)\n",
    "\n",
    "for i in range(env.get_size()):\n",
    "    for j in range(env.get_size()):\n",
    "\n",
    "        if env.is_terminal_state(i, j):\n",
    "            policy[i, j] = \"G\"\n",
    "            continue\n",
    "\n",
    "        action_values = []\n",
    "\n",
    "        for a in range(len(env.get_actions())):\n",
    "            next_i, next_j, reward, done = env.step(a, i, j)\n",
    "            action_value = reward + gamma * V[next_i, next_j]\n",
    "            action_values.append(action_value)\n",
    "\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy[i, j] = env.action_description[best_action]\n",
    "\n",
    "print(\"\\nOptimal Value Function (V*):\")\n",
    "print(np.round(V, 2))\n",
    "\n",
    "print(\"\\nOptimal Policy (π*):\")\n",
    "print(policy)\n"
   ],
   "id": "ff814ff7c2bcac2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 9 iterations\n",
      "\n",
      "Optimal Value Function (V*):\n",
      "[[-0.43  0.63  1.81  3.12  4.58]\n",
      " [ 0.63  1.81  3.12  4.58  6.2 ]\n",
      " [ 1.81  3.12  4.58  6.2   8.  ]\n",
      " [ 3.12  4.58  6.2   8.   10.  ]\n",
      " [ 4.58  6.2   8.   10.    0.  ]]\n",
      "\n",
      "Optimal Policy (π*):\n",
      "[['Right' 'Right' 'Right' 'Down' 'Down']\n",
      " ['Right' 'Right' 'Right' 'Right' 'Down']\n",
      " ['Right' 'Down' 'Right' 'Right' 'Down']\n",
      " ['Right' 'Right' 'Right' 'Right' 'Down']\n",
      " ['Right' 'Right' 'Right' 'Right' 'G']]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 2 In-Place Value Iteration\n",
    "- in standard Value Iteration two arrays are maintained\n",
    "  - Vk (old Values)\n",
    "  - Vk +1 (New Values)\n",
    "- All updates are computed using only the previous iterations values this is known as a synchronous update\n"
   ],
   "id": "b18cfee67b7a4563"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:04:27.376560Z",
     "start_time": "2026-02-11T00:04:27.356556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "# -------------------------------\n",
    "# Task 2: In-Place Value Iteration\n",
    "# -------------------------------\n",
    "\n",
    "V_inplace = np.zeros_like(V)\n",
    "iterations_inplace = 0\n",
    "converged = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while not converged:\n",
    "    delta = 0\n",
    "\n",
    "    for i in range(env.get_size()):\n",
    "        for j in range(env.get_size()):\n",
    "\n",
    "            if env.is_terminal_state(i, j):\n",
    "                continue\n",
    "\n",
    "            old_value = V_inplace[i, j]\n",
    "\n",
    "            action_values = []\n",
    "\n",
    "            for a in range(len(env.get_actions())):\n",
    "                next_i, next_j, reward, done = env.step(a, i, j)\n",
    "                action_value = reward + gamma * V_inplace[next_i, next_j]\n",
    "                action_values.append(action_value)\n",
    "\n",
    "            V_inplace[i, j] = max(action_values)\n",
    "\n",
    "            delta = max(delta, abs(old_value - V_inplace[i, j]))\n",
    "\n",
    "    iterations_inplace += 1\n",
    "\n",
    "    if delta < theta:\n",
    "        converged = True\n",
    "\n",
    "end_time = time.time()\n",
    "runtime_inplace = end_time - start_time\n",
    "\n",
    "# -------------------------------\n",
    "# Extract Policy from In-Place V*\n",
    "# -------------------------------\n",
    "\n",
    "policy_inplace = np.empty((env.get_size(), env.get_size()), dtype=object)\n",
    "\n",
    "for i in range(env.get_size()):\n",
    "    for j in range(env.get_size()):\n",
    "\n",
    "        if env.is_terminal_state(i, j):\n",
    "            policy_inplace[i, j] = \"G\"\n",
    "            continue\n",
    "\n",
    "        action_values = []\n",
    "\n",
    "        for a in range(len(env.get_actions())):\n",
    "            next_i, next_j, reward, done = env.step(a, i, j)\n",
    "            action_value = reward + gamma * V_inplace[next_i, next_j]\n",
    "            action_values.append(action_value)\n",
    "\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy_inplace[i, j] = env.action_description[best_action]\n",
    "\n",
    "# -------------------------------\n",
    "# Comparison Results\n",
    "# -------------------------------\n",
    "\n",
    "print(\"In-Place VI Converged in\", iterations_inplace, \"iterations\")\n",
    "print(\"Runtime:\", round(runtime_inplace, 6), \"seconds\")\n",
    "\n",
    "print(\"\\nOptimal Value Function (In-Place V*):\")\n",
    "print(np.round(V_inplace, 2))\n",
    "\n",
    "print(\"\\nMaximum difference between standard and in-place V*:\",\n",
    "      np.max(np.abs(V - V_inplace)))\n",
    "\n",
    "print(\"\\nDo policies differ?\",\n",
    "      np.any(policy != policy_inplace))\n"
   ],
   "id": "863b1ead93bfc4bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-Place VI Converged in 9 iterations\n",
      "Runtime: 0.003016 seconds\n",
      "\n",
      "Optimal Value Function (In-Place V*):\n",
      "[[-0.43  0.63  1.81  3.12  4.58]\n",
      " [ 0.63  1.81  3.12  4.58  6.2 ]\n",
      " [ 1.81  3.12  4.58  6.2   8.  ]\n",
      " [ 3.12  4.58  6.2   8.   10.  ]\n",
      " [ 4.58  6.2   8.   10.    0.  ]]\n",
      "\n",
      "Maximum difference between standard and in-place V*: 0.0\n",
      "\n",
      "Do policies differ? False\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The in-place variation converges to the same optimal value function as the standard synchronous Value iteration\n",
    "\n",
    "The maximum difference between the two computed value functions is approximately zero(within numerical tolerance, confirming correctness\n",
    "\n",
    "However, the number of iterations and runtime amy differ. In place updates often propagate value information faster, potentially reducing convergence time while maintain the same computational complexity per iteration\n",
    "This means both methods react the same fixed point V* but their convergence behaviour differs due to update ordering\n"
   ],
   "id": "2fab63d187dc6193"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:04:27.392160Z",
     "start_time": "2026-02-11T00:04:27.388109Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "71e24ad990c13a26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:04:27.415011Z",
     "start_time": "2026-02-11T00:04:27.411634Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5f6686079863f645",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T00:04:27.426373Z",
     "start_time": "2026-02-11T00:04:27.423026Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "50a8a4ec131ad984",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
